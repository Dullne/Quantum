{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 变分量子态对角化算法\n",
    "\n",
    "<em> Copyright (c) 2021 Institute for Quantum Computing, Baidu Inc. All Rights Reserved. </em>\n",
    "\n",
    "## 概览\n",
    "\n",
    "- 在本案例中，我们将通过 Paddle Quantum 训练量子神经网络来完成量子态的对角化\n",
    "\n",
    "- 首先，让我们通过下面几行代码引入必要的 library 和 package。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T10:39:04.699669Z",
     "start_time": "2021-01-09T10:38:59.947656Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numpy import diag\n",
    "import scipy\n",
    "from paddle import fluid\n",
    "from paddle_quantum.circuit import UAnsatz\n",
    "from paddle_quantum.utils import dagger\n",
    "from paddle.complex import matmul, trace, transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景\n",
    "量子态对角化算法（Variational Quantum State Diagonalization, VQSD）[1-3] 目标是输出一个量子态的本征谱，即其所有本征值。求解量子态的本征值在量子计算中有着诸多应用，比如可以用于计算保真度和冯诺依曼熵，也可以用于主成分分析。\n",
    "- 量子态通常是一个混合态，表示如下 $\\rho_{\\text{mixed}} = \\sum_i P_i |\\psi_i\\rangle\\langle\\psi_i|$\n",
    "- 作为一个简单的例子，我们考虑一个 2 量子位的量子态，它的本征谱为 $(0.5, 0.3, 0.1, 0.1)$，我们先通过随机作用一个酉矩阵来生成具有这样本征谱的随机量子态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T10:39:04.723926Z",
     "start_time": "2021-01-09T10:39:04.702772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2569-0.j     -0.012 +0.0435j -0.0492-0.0055j -0.0548+0.0682j]\n",
      " [-0.012 -0.0435j  0.2959+0.j      0.1061-0.0713j -0.0392-0.0971j]\n",
      " [-0.0492+0.0055j  0.1061+0.0713j  0.2145-0.j      0.0294-0.1132j]\n",
      " [-0.0548-0.0682j -0.0392+0.0971j  0.0294+0.1132j  0.2327+0.j    ]]\n"
     ]
    }
   ],
   "source": [
    "scipy.random.seed(13)                  # 固定随机种子，方便复现结果\n",
    "V = scipy.stats.unitary_group.rvs(4)   # 随机生成一个酉矩阵\n",
    "D = diag([0.5, 0.3, 0.1, 0.1])         # 输入目标态 rho 的谱\n",
    "V_H = V.conj().T \n",
    "rho = V @ D @ V_H                      # 通过逆向的谱分解生成 rho\n",
    "print(numpy.around(rho, 4))            # 打印量子态 rho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建量子神经网络\n",
    "- 在这个案例中，我们将通过训练量子神经网络 QNN（也可以理解为参数化量子电路）来学习量子态的本征谱。这里，我们提供一个预设的 2 量子位量子电路。\n",
    "\n",
    "- 我们预设一些该参数化电路的参数，比如宽度为 2 量子位。\n",
    "\n",
    "- 初始化其中的变量参数，${\\bf{\\theta }}$ 代表我们量子神经网络中的参数组成的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T10:39:04.741146Z",
     "start_time": "2021-01-09T10:39:04.729478Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 2            # 量子神经网络的宽度\n",
    "SEED = 14        # 固定随机种子\n",
    "THETA_SIZE = 15  # 量子神经网络中参数的数量\n",
    "\n",
    "def U_theta(theta, N):\n",
    "    \"\"\"\n",
    "    Quantum Neural Network\n",
    "    \"\"\"\n",
    "    \n",
    "    # 按照量子比特数量/网络宽度初始化量子神经网络\n",
    "    cir = UAnsatz(N)\n",
    "    # 调用内置的量子神经网络模板\n",
    "    cir.universal_2_qubit_gate(theta, [0, 1])\n",
    "    # 返回量子神经网络所模拟的酉矩阵 U\n",
    "    return cir.U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置训练模型——损失函数\n",
    "- 现在我们已经有了数据和量子神经网络的架构，我们将进一步定义训练参数、模型和损失函数。\n",
    "- 通过作用量子神经网络 $U(\\theta)$ 在量子态 $\\rho$ 后得到的量子态记为 $\\tilde\\rho$，我们设定损失函数为 $\\tilde\\rho$ 与用来标记的量子态 $\\sigma=0.1 |00\\rangle\\langle 00| + 0.2 |01\\rangle \\langle 01| + 0.3 |10\\rangle \\langle10| + 0.4 |11 \\rangle\\langle 11|$ 的内积。\n",
    "- 具体的，设定损失函数为 $\\mathcal{L}(\\boldsymbol{\\theta})  = \\text{Tr}(\\tilde\\rho\\sigma)$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T10:39:04.763989Z",
     "start_time": "2021-01-09T10:39:04.749814Z"
    }
   },
   "outputs": [],
   "source": [
    "# 输入用来标记的量子态sigma\n",
    "sigma = diag([0.1, 0.2, 0.3, 0.4]).astype('complex128') \n",
    "\n",
    "class Net(fluid.dygraph.Layer):\n",
    "    \"\"\"\n",
    "    Construct the model net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, shape, rho, sigma, param_attr =fluid.initializer.Uniform(low=0.0, high=2 * numpy.pi, seed=SEED), dtype='float64'):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 将 numpy.ndarray 转换成 Paddle 动态图模式中支持的 variable\n",
    "        self.rho = fluid.dygraph.to_variable(rho)\n",
    "        self.sigma = fluid.dygraph.to_variable(sigma)\n",
    "        \n",
    "        # 初始化 theta 参数列表，并用 [0, 2*pi] 的均匀分布来填充初始值\n",
    "        self.theta = self.create_parameter(\n",
    "        shape=shape, attr=param_attr, dtype=dtype, is_bias=False)\n",
    "\n",
    "    # 定义损失函数和前向传播机制\n",
    "    def forward(self, N):\n",
    "        \n",
    "        # 施加量子神经网络\n",
    "        U = U_theta(self.theta, N)\n",
    "\n",
    "        # rho_tilde 是将 U 作用在 rho 后得到的量子态 U*rho*U^dagger \n",
    "        rho_tilde = matmul(matmul(U, self.rho), dagger(U))\n",
    "\n",
    "        # 计算损失函数\n",
    "        loss = trace(matmul(self.sigma, rho_tilde))\n",
    "\n",
    "        return loss.real, rho_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置训练模型——模型参数\n",
    "\n",
    "在进行量子神经网络的训练之前，我们还需要进行一些训练的超参数设置，主要是学习速率 (learning rate, LR)和迭代次数(iteration, ITR)。这里我们设定学习速率为 0.1，迭代次数为 50 次。读者不妨自行调整来直观感受下超参数调整对训练效果的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T10:39:04.782143Z",
     "start_time": "2021-01-09T10:39:04.771644Z"
    }
   },
   "outputs": [],
   "source": [
    "ITR = 50  # 设置训练的总的迭代次数\n",
    "LR = 0.1  # 设置学习速率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行训练\n",
    "\n",
    "- 当训练模型的各项参数都设置完成后，我们将数据转化为 Paddle 动态图中的变量，进而进行量子神经网络的训练。\n",
    "- 过程中我们用的是 Adam Optimizer，也可以调用 Paddle 中提供的其他优化器。\n",
    "- 我们将训练过程中的结果依次输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T10:39:06.249584Z",
     "start_time": "2021-01-09T10:39:04.786391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 loss: 0.2354\n",
      "iter: 10 loss: 0.1883\n",
      "iter: 20 loss: 0.1817\n",
      "iter: 30 loss: 0.1814\n",
      "iter: 40 loss: 0.1809\n"
     ]
    }
   ],
   "source": [
    "# 初始化paddle动态图机制\n",
    "with fluid.dygraph.guard():\n",
    "    \n",
    "    # 确定网络的参数维度\n",
    "    net = Net(shape=[THETA_SIZE], rho=rho, sigma=sigma)\n",
    "\n",
    "    # 一般来说，我们利用 Adam 优化器来获得相对好的收敛\n",
    "    # 当然你可以改成 SGD 或者是 RMS prop.\n",
    "    opt = fluid.optimizer.AdamOptimizer(learning_rate=LR, parameter_list=net.parameters())\n",
    "    \n",
    "    # 优化循环\n",
    "    for itr in range(ITR):\n",
    "        \n",
    "        # 前向传播计算损失函数并返回估计的能谱\n",
    "        loss, rho_tilde = net(N)\n",
    "        rho_tilde_np = rho_tilde.numpy()\n",
    "        \n",
    "        # 在动态图机制下，反向传播极小化损失函数\n",
    "        loss.backward()\n",
    "        opt.minimize(loss)\n",
    "        net.clear_gradients()\n",
    "        \n",
    "        # 打印训练结果\n",
    "        if itr % 10 == 0:\n",
    "            print('iter:', itr, 'loss:', '%.4f' % loss.numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "根据上面训练得到的结果，通过大概50次迭代，我们就比较好的完成了对角化。我们可以通过打印 $\\tilde{\\rho} = U(\\boldsymbol{\\theta})\\rho U^\\dagger(\\boldsymbol{\\theta})$ 的来验证谱分解的效果。特别的，我们可以验证它的对角线与目标谱非常接近。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T10:39:06.263643Z",
     "start_time": "2021-01-09T10:39:06.254056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated spectrum is: [0.49951803 0.29739084 0.10256743 0.10052371]\n",
      "The target spectrum is: [0.5 0.3 0.1 0.1]\n"
     ]
    }
   ],
   "source": [
    "print(\"The estimated spectrum is:\", numpy.real(numpy.diag(rho_tilde_np)))\n",
    "print(\"The target spectrum is:\", numpy.diag(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "[1] Larose, R., Tikku, A., Neel-judy, É. O., Cincio, L. & Coles, P. J. Variational quantum state diagonalization. [npj Quantum Inf. (2019) doi:10.1038/s41534-019-0167-6.](https://www.nature.com/articles/s41534-019-0167-6)\n",
    "\n",
    "[2] Nakanishi, K. M., Mitarai, K. & Fujii, K. Subspace-search variational quantum eigensolver for excited states. [Phys. Rev. Res. 1, 033062 (2019).](https://journals.aps.org/prresearch/pdf/10.1103/PhysRevResearch.1.033062)\n",
    "\n",
    "[3] Cerezo, M., Sharma, K., Arrasmith, A. & Coles, P. J. Variational Quantum State Eigensolver. [arXiv:2004.01372 (2020).](https://arxiv.org/pdf/2004.01372.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
